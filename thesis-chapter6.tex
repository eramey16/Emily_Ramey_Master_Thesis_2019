\chapter{Conclusion}

\section{Discussion}
As part of this project, we developed and tested a model simulator for gamma-ray trajectories, wrote a Compton reconstruction algorithm, and set up a framework for testing and investigating trends in our data. We have met our constraints for power and better constrained our accuracy and reconstruction speeds based on possible input parameters and simulated environmental factors.

Our overall goal was to be able to reconstruct at least 50,000 photons per second with greater than 75\% accuracy, and, based on our findings in sections \ref{totalhits} and \ref{reconhits}, we will be able to achieve this goal by implementing a maximum cutoff in the number of reconstructed hits for each event. Based on our data in Figure \ref{fig:hits_speed}, we have also been able to improve the average-case time complexity of our program from factorial to exponential with our tree search algorithm. Our program has, on average, a linear trend in speedup, indicating that it scales well, though we will likely have a maximum of four cores to work with if the telescope uses a standard ARM processor in its hardware.

Though we saw mostly expected behavior when testing the various parameters, there are a few interesting features which may prove useful to future work on this project. In varying the number of hits for each event, we found an unexpected trend in accuracy that may help us better reconstruct the position of a gamma-ray source in future iterations of the project. While investigating reconstructed hits, we found that stopping computation early could save us computation time without significant loss of accuracy. Finally, in varying our predicted noise levels, we found that the point at which the accuracy flattens out is a fairly good predictor of the simulated energy noise in our system. This could help us better determine the true noise level when performing tests with real telescope equipment.

\section{Future Work}
Though this project laid the groundwork for the simulation and reconstruction of Compton scatters for our detector, there is much left to do before the telescope becomes fully operational. One of the primary goals of future projects will be to repeat the test cases we have performed on more accurate data from CERN's Geant 4 simulator to make sure that we see the same behavior from both. An alternative approach could be to make our toy model simulator more robust by simulating events from a given gamma-ray source profile and by including the correct probability distributions for Compton scattering angles. Once this is finished, the next logical step would be to include the Klein-Nishina formula as a prior on our reconstruction algorithm, which would prioritize triples of hits more likely to be Compton scatters based on the angle itself, rather than just comparing the energy and spatial angles to see if they match. Compton scatters are much more likely to happen at small angles for our energy range, so including this in our calculations could further improve our reconstruction accuracy.

Testing on a better simulator would also allow us to examine the overall accuracy of our program for different source distributions rather than just the accuracy for events with a given number of hits. In our current tests, all events have equal weight, whereas Compton scatters in nature are much more likely to have 2-3 hits than any larger number. This means that the overall accuracy would be a weighted mean of those we found in section \ref{totalhits}, with the overall shape of the distribution depending on the emission distribution of the source. With these probabilities taken into account, we would potentially be able to optimize our program for the expected source distribution. Correct figures for the accuracy would also be very useful as a prior on the source reconstruction algorithm, which has yet to be developed.

The use of parallelism in our algorithm also requires some further investigation. The current program runs each photon in parallel, but theoretically each branch of our tree search could also be done simultaneously. There would be a small amount of overhead associated with the creation of each parallel thread, so at some point the extra parallelism would cease to be worth the decreased speed. Though we are meeting our speed goals currently, it would be interesting to investigate the optimal level of parallelism for a program such as this, in case it is needed in the future. 